{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例子：卖煎饼，目标是卖更多的煎饼果子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are wrong but some are useful. -- George E.P. Box\n",
    "\n",
    "建模：留下明显特征，在特征的选择时有一个问题：Curse of Dimensionality，在涉及到向量的计算的问题中，随着维数的增加，计算量呈指数倍增长的一种现象。维数灾难涉及数字分析、抽样、组合、机器学习、数据挖掘和数据库等诸多领域。特征工程，从很多特征中选出一些特征。（深度学习解决这个问题。）\n",
    "\n",
    "方案一：第一本书（$k$-near neighbor）\n",
    "\n",
    "气温、沙尘暴、PM2.5、日期、卖了多久（特征）；学校（Target）\n",
    "\n",
    "好处：知道哪天去哪好，直接查表（超过内存，计算机回会比较慢）；\n",
    "\n",
    "坏处：太厚了；抽象层次不够（不好处理一些没出现的问题）；\n",
    "\n",
    "方案二：第二本书\n",
    "\n",
    "关于决策的树：决策树，优先考虑区分度大的特征\n",
    "\n",
    "好处：不断缩小查询范围，决策过程清晰明了。\n",
    "\n",
    "方案三：第三本书（基于概率的模型）\n",
    "\n",
    "朴素贝叶斯分类：统计过去的数据\n",
    "\n",
    "方案四：第四本书\n",
    "\n",
    "经验丰富：Neural Network，不断考虑权值的变化，激活单元。\n",
    "（深度学习模型和概率模型可以结合在一起。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine if is a valuable customer in Wechat?\n",
    "2. What Features do we need?\n",
    "3. How to predicate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果刚开始投放：给人贴标签，直接根据标签推荐。地理位置，人物性格，转账情况等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Classification\n",
    "2. Regression：预测\n",
    "\n",
    "都预测为 1,2,3：分类只代表差异，预测是有相关性的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习问题如何衡量？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与管理系统类似，机器要自己调整，那么目标是什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "粗糙的来说，这是给人看的：Prediction（抓的人的准确率），Recall（所有坏人中抓到了多少：覆盖率）\n",
    "\n",
    "$$F_1=\\frac{Precision\\times Recall}{Precision + Recall}.$$\n",
    "\n",
    "更全面的衡量，类似还有 $F_2$.\n",
    "\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欠拟合和过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程的 ACC 很低：欠拟合（underfitting），可能的原因，模型简单刻画不了复杂的模型。\n",
    "\n",
    "过拟合：训练的数据效果好，但是真正的数据效果不好，原因有几个：\n",
    "1. 数据过少；\n",
    "2. 数据噪声，异常值；\n",
    "3. 模型太复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关于过拟合和欠拟合的一些注记。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠拟合是模型不能再训练集上获得足够低的误差，而过拟合则是训练误差和测试误差之间的差距太大。一个影响因素是容量（capacity），也就是模型空间拟合各种函数的能力。如果容量低，则可能出现模型难以拟合训练集的情况，导致欠拟合（高偏差）；如果容量高，则因为模型记住了不适用于测试集的训练集性质，导致过拟合（高方差）。\n",
    "\n",
    "我们偏向喜欢简单的模型，有三点原因：\n",
    "1. 预测准确度：复杂模型需要更多的数据。\n",
    "2. 模型的可解释性：比如线性回归\n",
    "\n",
    "$$Y=\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p+\\epsilon.$$\n",
    "\n",
    "这里面 0 的数量比较多的话，则说明哪些是主要因素。\n",
    "3. 模型的稳定性：额外增加一些数据不会产生大的变动。\n",
    "\n",
    "获得简单模型的办法：\n",
    "1. 特征选择（特征工程）：前向选择和后向选择\n",
    "2. 正则化：Ridge（岭回归），Lasso 等\n",
    "3. 降维：主值分解\n",
    "\n",
    "其中前两点从 $\\beta_i$ 入手，而第三点从 $X_i$ 入手。\n",
    "我们在处理过拟合和欠拟合的问题时，实际上就是在权衡偏差和方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以线性回归为例：\n",
    "\n",
    "$$\\mathrm{RSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}$$\n",
    "\n",
    "目标是惩罚大的系数，也就是最小化：\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}=\\mathrm{RSS}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}.\n",
    "$$\n",
    "\n",
    "其中 $\\lambda$ 是一个可调参数。\n",
    "\n",
    "Ridge正则化的概率论观点：\n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) \\propto p(\\mathcal{D} | \\theta) p(\\theta).\n",
    "$$\n",
    "\n",
    "对于系数本身先验第服从指定宽度的 Gauss 分布：\n",
    "$$\n",
    "p\\left(\\beta | \\tau^{2}\\right)=\\prod_{{j}=1}^{{p}} \\mathcal{N}\\left(\\beta_{j} | 0, \\tau^{2}\\right).\n",
    "$$\n",
    "\n",
    "从而，\n",
    "$$\n",
    "\\begin{aligned} \\log p(\\beta | \\mathcal{D}) & \\propto \\log (p(\\mathcal{D} | \\beta) \\mathbf{p}(\\beta)) \\\\ &=\\log \\left[\\left(\\prod_{i=1}^{n} \\mathcal{N}\\left(y_{i} | x_{i}^{T} \\beta, \\sigma^{2}\\right)\\right) \\prod_{j=1}^{p} \\mathcal{N}\\left(\\beta_{\\mathbf{j}} | \\mathbf{0}, \\tau^{2}\\right)\\right] \\\\ &=\\log \\left[\\left(\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left\\{\\frac{-1}{2 \\sigma^{2}}\\left(y_{i}-\\beta^{\\top} \\mathbf{x}_{\\mathbf{i}}\\right)^{2}\\right\\}\\right) \\prod_{j=1}^{p} \\log \\mathcal{N}\\left(\\beta_{j} | 0, \\tau^{2}\\right)\\right.\\\\ &=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\beta^{\\top} \\mathbf{x}_{\\mathbf{i}}\\right)^{2}+\\frac{\\sigma^{2}}{\\tau^{2}} \\sum_{p=1}^{j} \\beta_{j}^{2} \\end{aligned}\n",
    "$$\n",
    "\n",
    "我们看出 $\\lambda=\\frac{\\sigma^{2}}{\\tau^{2}}$.\n",
    "如果 $\\lambda$ 很小，则可能发生过拟合；如果 $\\lambda$ 很大，则容易发生欠拟合。随着 $\\lambda$ 的变化，不同系数的变化情况是不一样的（整体上都是变小的，意味着不同系数对于结果的影响是不一样的），根据 $\\lambda$ 变化绘制的系数变化的曲线叫做正则化路径曲线."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是以线性回归为例：\n",
    "$$\n",
    "Y=\\beta_{0}+\\beta_{1} X_{1}+\\cdots+\\beta_{p} X_{p}+\\epsilon.\n",
    "$$\n",
    "\n",
    "Lasso 正则化的一个重要特点就是：有很多系数为 0，从而有以下特点\n",
    "\n",
    "1. 自动完成了特征筛选（特征工程）\n",
    "2. 避免了过拟合\n",
    "3. 更快的运算速度\n",
    "4. 可解释性\n",
    "\n",
    "Lasso 惩罚了系数的 $L_1$ 长度：\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}=\\arg \\min _{\\beta \\in \\mathcal{B}}\\left\\{\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{j=1}^{p} \\beta_{j} x_{i, j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|\\right\\}.\n",
    "$$\n",
    "\n",
    "如果观察 Lasso 的正则化路径就会发现，真的有一些系数变为 0 了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较 Ridge 和 Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lasso 具有系数稀疏性，而 Ridge 和 $L_0$ 正则化都没有系数的稀疏性。\n",
    "2. 从先验概率来说， Radge 正则化的系数是关于 $y$ 轴对称的 Gauss 分布，而 Lasso 正则化则是关于 $y$ 轴对称的 尖点分布，这也结识了 Lasso 的稀疏性的特点。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
